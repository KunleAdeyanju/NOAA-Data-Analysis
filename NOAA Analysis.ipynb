{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\">NOAA Data Analysis</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 - Gathering data\n",
    "\n",
    "Automating API calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all libraries\n",
    "import urllib.request\n",
    "import urllib.error\n",
    "import json\n",
    "import os\n",
    "from key import token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_api(url, token):\n",
    "    \"\"\"\n",
    "        Make a GET request to the API and return a JSON response\n",
    "\n",
    "        Args:\n",
    "            url (str): The API endpoint URL\n",
    "            token (str): The API token for authentication\n",
    "        Returns:\n",
    "            dict: The parsed JSON response from the API\n",
    "        Summary of What Happens\n",
    "        1. A Request object is created for the URL.\n",
    "        2. A custom header (token) is added to the request.\n",
    "        3. The request is sent to the server using urlopen.\n",
    "        4. The server's response is read, decoded from bytes to a string, and parsed as JSON.\n",
    "        5. The resulting Python object (data) contains the parsed JSON data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        req = urllib.request.Request(url)\n",
    "        req.add_header('token', token)\n",
    "        with urllib.request.urlopen(req) as response:\n",
    "            data = json.loads(response.read().decode('utf-8'))\n",
    "            # Get the remaining rate limit from the headers\n",
    "            rate_limit_remaining = int(response.headers.get('X-RateLimit-Remaining', 0))\n",
    "            return data, rate_limit_remaining\n",
    "\n",
    "    except urllib.error.HTTPError as e:\n",
    "        # Handle HTTP errors (e.g., 404, 401)\n",
    "        print(f\"HTTP Error: {e.code} - {e.reason}\")\n",
    "    except urllib.error.URLError as e:\n",
    "        # Handle URL errors (e.g., network issues)\n",
    "        print(f\"URL Error: {e.reason}\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        # Handle JSON parsing errors\n",
    "        print(f\"JSON Decode Error: {e.msg}\")\n",
    "    except Exception as e:\n",
    "        # Handle any other unexpected errors\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "    return None,0  # Return None if an error occurs\n",
    "\n",
    "def save_json_to_file(data, filename):\n",
    "    \"\"\"\n",
    "        Save JSON data to a file\n",
    "\n",
    "        Args:\n",
    "            data (dict): The JSON data to save\n",
    "            filename (str): The name of the file to save the data to\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(f'./data/{filename}', 'w') as f:\n",
    "            json.dump(data, f, indent=4) # Pretty print the JSON, the indentation level is 4 spaces\n",
    "            print(f'Data saved to {filename}')\n",
    "    except IOError as e:\n",
    "        # Handle file I/O errors\n",
    "        print(f'File I/O Error: {e}')\n",
    "    except Exception as e:\n",
    "        # Handle any other unexpected errors\n",
    "        print(f'An unexpected error occurred while saving the file: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling API with: https://www.ncdc.noaa.gov/cdo-web/api/v2/locations?limit=1000&offset=0\n",
      "Fetched 1000 records in this batch.\n",
      "Data saved to locations_0.json\n",
      "Data saved to locations_0.json\n",
      "Calling API with: https://www.ncdc.noaa.gov/cdo-web/api/v2/locations?limit=1000&offset=1000\n",
      "Fetched 1000 records in this batch.\n",
      "Data saved to locations_1.json\n",
      "Data saved to locations_1.json\n",
      "Calling API with: https://www.ncdc.noaa.gov/cdo-web/api/v2/locations?limit=1000&offset=2000\n",
      "Fetched 1000 records in this batch.\n",
      "Data saved to locations_2.json\n",
      "Data saved to locations_2.json\n",
      "Calling API with: https://www.ncdc.noaa.gov/cdo-web/api/v2/locations?limit=1000&offset=3000\n",
      "Fetched 1000 records in this batch.\n",
      "Data saved to locations_3.json\n",
      "Data saved to locations_3.json\n",
      "Calling API with: https://www.ncdc.noaa.gov/cdo-web/api/v2/locations?limit=1000&offset=4000\n",
      "Fetched 1000 records in this batch.\n",
      "Data saved to locations_4.json\n",
      "Data saved to locations_4.json\n",
      "Calling API with: https://www.ncdc.noaa.gov/cdo-web/api/v2/locations?limit=1000&offset=5000\n",
      "Fetched 1000 records in this batch.\n",
      "Data saved to locations_5.json\n",
      "Data saved to locations_5.json\n",
      "Calling API with: https://www.ncdc.noaa.gov/cdo-web/api/v2/locations?limit=1000&offset=6000\n",
      "Fetched 1000 records in this batch.\n",
      "Data saved to locations_6.json\n",
      "Data saved to locations_6.json\n",
      "Calling API with: https://www.ncdc.noaa.gov/cdo-web/api/v2/locations?limit=1000&offset=7000\n",
      "Fetched 1000 records in this batch.\n",
      "Data saved to locations_7.json\n",
      "Data saved to locations_7.json\n",
      "Calling API with: https://www.ncdc.noaa.gov/cdo-web/api/v2/locations?limit=1000&offset=8000\n",
      "Fetched 1000 records in this batch.\n",
      "Data saved to locations_8.json\n",
      "Data saved to locations_8.json\n",
      "Calling API with: https://www.ncdc.noaa.gov/cdo-web/api/v2/locations?limit=1000&offset=9000\n",
      "Fetched 1000 records in this batch.\n",
      "Data saved to locations_9.json\n",
      "Data saved to locations_9.json\n",
      "Calling API with: https://www.ncdc.noaa.gov/cdo-web/api/v2/locations?limit=1000&offset=10000\n",
      "Fetched 1000 records in this batch.\n",
      "Data saved to locations_10.json\n",
      "Data saved to locations_10.json\n",
      "Calling API with: https://www.ncdc.noaa.gov/cdo-web/api/v2/locations?limit=1000&offset=11000\n",
      "Fetched 1000 records in this batch.\n",
      "Data saved to locations_11.json\n",
      "Data saved to locations_11.json\n",
      "Calling API with: https://www.ncdc.noaa.gov/cdo-web/api/v2/locations?limit=1000&offset=12000\n",
      "Fetched 1000 records in this batch.\n",
      "Data saved to locations_12.json\n",
      "Data saved to locations_12.json\n",
      "Calling API with: https://www.ncdc.noaa.gov/cdo-web/api/v2/locations?limit=1000&offset=13000\n",
      "Fetched 1000 records in this batch.\n",
      "Data saved to locations_13.json\n",
      "Data saved to locations_13.json\n",
      "Calling API with: https://www.ncdc.noaa.gov/cdo-web/api/v2/locations?limit=1000&offset=14000\n",
      "Fetched 1000 records in this batch.\n",
      "Data saved to locations_14.json\n",
      "Data saved to locations_14.json\n",
      "Calling API with: https://www.ncdc.noaa.gov/cdo-web/api/v2/locations?limit=1000&offset=15000\n",
      "Fetched 1000 records in this batch.\n",
      "Data saved to locations_15.json\n",
      "Data saved to locations_15.json\n",
      "Calling API with: https://www.ncdc.noaa.gov/cdo-web/api/v2/locations?limit=1000&offset=16000\n",
      "Fetched 1000 records in this batch.\n",
      "Data saved to locations_16.json\n",
      "Data saved to locations_16.json\n",
      "Calling API with: https://www.ncdc.noaa.gov/cdo-web/api/v2/locations?limit=1000&offset=17000\n",
      "Fetched 1000 records in this batch.\n",
      "Data saved to locations_17.json\n",
      "Data saved to locations_17.json\n",
      "Calling API with: https://www.ncdc.noaa.gov/cdo-web/api/v2/locations?limit=1000&offset=18000\n",
      "Fetched 1000 records in this batch.\n",
      "Data saved to locations_18.json\n",
      "Data saved to locations_18.json\n",
      "Calling API with: https://www.ncdc.noaa.gov/cdo-web/api/v2/locations?limit=1000&offset=19000\n",
      "Fetched 1000 records in this batch.\n",
      "Data saved to locations_19.json\n",
      "Data saved to locations_19.json\n",
      "Calling API with: https://www.ncdc.noaa.gov/cdo-web/api/v2/locations?limit=1000&offset=20000\n",
      "Fetched 1000 records in this batch.\n",
      "Data saved to locations_20.json\n",
      "Data saved to locations_20.json\n",
      "Calling API with: https://www.ncdc.noaa.gov/cdo-web/api/v2/locations?limit=1000&offset=21000\n",
      "Fetched 1000 records in this batch.\n",
      "Data saved to locations_21.json\n",
      "Data saved to locations_21.json\n",
      "Calling API with: https://www.ncdc.noaa.gov/cdo-web/api/v2/locations?limit=1000&offset=22000\n",
      "Fetched 1000 records in this batch.\n",
      "Data saved to locations_22.json\n",
      "Data saved to locations_22.json\n",
      "Calling API with: https://www.ncdc.noaa.gov/cdo-web/api/v2/locations?limit=1000&offset=23000\n",
      "Fetched 1000 records in this batch.\n",
      "Data saved to locations_23.json\n",
      "Data saved to locations_23.json\n",
      "Calling API with: https://www.ncdc.noaa.gov/cdo-web/api/v2/locations?limit=1000&offset=24000\n",
      "Fetched 1000 records in this batch.\n",
      "Data saved to locations_24.json\n",
      "Data saved to locations_24.json\n",
      "Calling API with: https://www.ncdc.noaa.gov/cdo-web/api/v2/locations?limit=1000&offset=25000\n",
      "Fetched 1000 records in this batch.\n",
      "Data saved to locations_25.json\n",
      "Data saved to locations_25.json\n",
      "Calling API with: https://www.ncdc.noaa.gov/cdo-web/api/v2/locations?limit=1000&offset=26000\n",
      "Fetched 1000 records in this batch.\n",
      "Data saved to locations_26.json\n",
      "Data saved to locations_26.json\n",
      "Calling API with: https://www.ncdc.noaa.gov/cdo-web/api/v2/locations?limit=1000&offset=27000\n",
      "Fetched 1000 records in this batch.\n",
      "Data saved to locations_27.json\n",
      "Data saved to locations_27.json\n",
      "Calling API with: https://www.ncdc.noaa.gov/cdo-web/api/v2/locations?limit=1000&offset=28000\n",
      "Fetched 1000 records in this batch.\n",
      "Data saved to locations_28.json\n",
      "Data saved to locations_28.json\n",
      "Calling API with: https://www.ncdc.noaa.gov/cdo-web/api/v2/locations?limit=1000&offset=29000\n",
      "Fetched 1000 records in this batch.\n",
      "Data saved to locations_29.json\n",
      "Data saved to locations_29.json\n",
      "Calling API with: https://www.ncdc.noaa.gov/cdo-web/api/v2/locations?limit=1000&offset=30000\n",
      "Fetched 1000 records in this batch.\n",
      "Data saved to locations_30.json\n",
      "Data saved to locations_30.json\n",
      "Calling API with: https://www.ncdc.noaa.gov/cdo-web/api/v2/locations?limit=1000&offset=31000\n",
      "Fetched 1000 records in this batch.\n",
      "Data saved to locations_31.json\n",
      "Data saved to locations_31.json\n",
      "Calling API with: https://www.ncdc.noaa.gov/cdo-web/api/v2/locations?limit=1000&offset=32000\n",
      "Fetched 1000 records in this batch.\n",
      "Data saved to locations_32.json\n",
      "Data saved to locations_32.json\n",
      "Calling API with: https://www.ncdc.noaa.gov/cdo-web/api/v2/locations?limit=1000&offset=33000\n",
      "Fetched 1000 records in this batch.\n",
      "Data saved to locations_33.json\n",
      "Data saved to locations_33.json\n",
      "Calling API with: https://www.ncdc.noaa.gov/cdo-web/api/v2/locations?limit=1000&offset=34000\n",
      "Fetched 1000 records in this batch.\n",
      "Data saved to locations_34.json\n",
      "Data saved to locations_34.json\n",
      "Calling API with: https://www.ncdc.noaa.gov/cdo-web/api/v2/locations?limit=1000&offset=35000\n",
      "Fetched 1000 records in this batch.\n",
      "Data saved to locations_35.json\n",
      "Data saved to locations_35.json\n",
      "Calling API with: https://www.ncdc.noaa.gov/cdo-web/api/v2/locations?limit=1000&offset=36000\n",
      "Fetched 1000 records in this batch.\n",
      "Data saved to locations_36.json\n",
      "Data saved to locations_36.json\n",
      "Calling API with: https://www.ncdc.noaa.gov/cdo-web/api/v2/locations?limit=1000&offset=37000\n",
      "Fetched 1000 records in this batch.\n",
      "Data saved to locations_37.json\n",
      "Data saved to locations_37.json\n",
      "Calling API with: https://www.ncdc.noaa.gov/cdo-web/api/v2/locations?limit=1000&offset=38000\n",
      "Fetched 863 records in this batch.\n",
      "Data saved to locations_38.json\n",
      "Data saved to locations_38.json\n",
      "No more records to fetch.\n"
     ]
    }
   ],
   "source": [
    "web_site = 'https://www.ncdc.noaa.gov/cdo-web/api/v2/locations'\n",
    "\n",
    "limit = 1000  # The maximum number of records to retrieve in one request\n",
    "offset_increment = limit  # Increment offset by the limit for each request\n",
    "\n",
    "\n",
    "\n",
    "for i in range(0,40):  # Loop 39 times to create 39 files\n",
    "    offset = i * offset_increment  # Calculate the offset for this iteration\n",
    "    url = f\"{web_site}?limit={limit}&offset={offset}\"  # Construct the URL\n",
    "    output_file = f\"locations_{i}.json\"  # Name the output file\n",
    "\n",
    "    print(f\"Calling API with: {url}\")\n",
    "    response, remaining_requests = call_api(url, token)\n",
    "\n",
    "        # Check if the response is valid\n",
    "    if response:\n",
    "        results = response.get('results', [])\n",
    "        print(f\"Fetched {len(results)} records in this batch.\")\n",
    "\n",
    "        # Save the response to a file\n",
    "        save_json_to_file(response, output_file)\n",
    "        print(f\"Data saved to {output_file}\")\n",
    "\n",
    "        # Stop if fewer records are returned than the limit\n",
    "        if len(results) < limit:\n",
    "            print(\"No more records to fetch.\")\n",
    "            break\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data for offset {offset}, exiting loop.\")\n",
    "        break\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 - Transforming Data\n",
    "\n",
    "Convert Json files to Pandas Dataframe\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_json(file_path):\n",
    "    '''\n",
    "        Read the json file from specified file path\n",
    "\n",
    "        Args:\n",
    "            file_path (str): the location of the file\n",
    "\n",
    "        Returns:\n",
    "            data(dict): the data stored in the json file \n",
    "    \n",
    "    '''\n",
    "    with open(file_path, 'r') as files:\n",
    "        data = json.load(files)\n",
    "    return data\n",
    "\n",
    "\n",
    "def read_all_json_files(root):\n",
    "    '''\n",
    "        Read all json files in a directory and store in a data frame\n",
    "\n",
    "        Args:\n",
    "            root(str): the directory where the files are stored\n",
    "\n",
    "        Returns:\n",
    "            json_df (pd.Dataframe): Pandas Dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    '''\n",
    "    json_df = pd.DataFrame() # create an empty DataFrame\n",
    "\n",
    "    # loop through all files in the given directory\n",
    "    for files in os.listdir(root):\n",
    "        full_path = f'{root}/{files}' # creates the full path for each file, os.path.join(root, files) also works\n",
    "\n",
    "        try:\n",
    "            data = read_json(full_path)\n",
    "\n",
    "            holder_df = pd.DataFrame(data['results']) # temp Dataframe to hold data until joined into master dataframe\n",
    "            holder_df['source'] = files # add 'source' column to track the data source\n",
    "            json_df = pd.concat([json_df,holder_df], ignore_index=True)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f'Error reading {full_path}: {e}')\n",
    "    return json_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mindate</th>\n",
       "      <th>maxdate</th>\n",
       "      <th>name</th>\n",
       "      <th>datacoverage</th>\n",
       "      <th>id</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1997-03-06</td>\n",
       "      <td>2025-07-28</td>\n",
       "      <td>Grand Isle, VT 05458</td>\n",
       "      <td>0.95</td>\n",
       "      <td>ZIP:05458</td>\n",
       "      <td>locations_10.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1948-05-01</td>\n",
       "      <td>2025-07-28</td>\n",
       "      <td>Highgate Center, VT 05459</td>\n",
       "      <td>1.00</td>\n",
       "      <td>ZIP:05459</td>\n",
       "      <td>locations_10.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1995-05-08</td>\n",
       "      <td>2025-07-28</td>\n",
       "      <td>Hinesburg, VT 05461</td>\n",
       "      <td>1.00</td>\n",
       "      <td>ZIP:05461</td>\n",
       "      <td>locations_10.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1955-11-01</td>\n",
       "      <td>2025-07-28</td>\n",
       "      <td>Huntington, VT 05462</td>\n",
       "      <td>1.00</td>\n",
       "      <td>ZIP:05462</td>\n",
       "      <td>locations_10.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1997-03-06</td>\n",
       "      <td>2025-07-28</td>\n",
       "      <td>Isle la Motte, VT 05463</td>\n",
       "      <td>0.95</td>\n",
       "      <td>ZIP:05463</td>\n",
       "      <td>locations_10.json</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      mindate     maxdate                       name  datacoverage         id  \\\n",
       "0  1997-03-06  2025-07-28       Grand Isle, VT 05458          0.95  ZIP:05458   \n",
       "1  1948-05-01  2025-07-28  Highgate Center, VT 05459          1.00  ZIP:05459   \n",
       "2  1995-05-08  2025-07-28        Hinesburg, VT 05461          1.00  ZIP:05461   \n",
       "3  1955-11-01  2025-07-28       Huntington, VT 05462          1.00  ZIP:05462   \n",
       "4  1997-03-06  2025-07-28    Isle la Motte, VT 05463          0.95  ZIP:05463   \n",
       "\n",
       "              source  \n",
       "0  locations_10.json  \n",
       "1  locations_10.json  \n",
       "2  locations_10.json  \n",
       "3  locations_10.json  \n",
       "4  locations_10.json  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check on the first few rows\n",
    "climate_data_df = read_all_json_files('./data')\n",
    "climate_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38863, 6)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# validate data was accurately aquried\n",
    "climate_data_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mindate</th>\n",
       "      <th>maxdate</th>\n",
       "      <th>name</th>\n",
       "      <th>datacoverage</th>\n",
       "      <th>id</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [mindate, maxdate, name, datacoverage, id, source]\n",
       "Index: []"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for duplicated rows\n",
    "dups = climate_data_df[climate_data_df.duplicated()] # filters Climate df to return only rows where .duplicated() is True\n",
    "dups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are duplicate entries in the 'id' column\n"
     ]
    }
   ],
   "source": [
    "# secondary check for unique\n",
    "\n",
    "if climate_data_df['id'].is_unique:\n",
    "    print(\"All entries in the 'id' column are unique\")\n",
    "else:\n",
    "    print(\"There are duplicate entries in the 'id' column\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mindate</th>\n",
       "      <th>maxdate</th>\n",
       "      <th>name</th>\n",
       "      <th>datacoverage</th>\n",
       "      <th>id</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30863</th>\n",
       "      <td>2003-03-01</td>\n",
       "      <td>2025-07-26</td>\n",
       "      <td>Kutahya, TU</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CITY:TU000037</td>\n",
       "      <td>locations_1.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36862</th>\n",
       "      <td>2003-03-01</td>\n",
       "      <td>2025-07-26</td>\n",
       "      <td>Kutahya, TU</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CITY:TU000037</td>\n",
       "      <td>locations_0.json</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          mindate     maxdate         name  datacoverage             id  \\\n",
       "30863  2003-03-01  2025-07-26  Kutahya, TU           1.0  CITY:TU000037   \n",
       "36862  2003-03-01  2025-07-26  Kutahya, TU           1.0  CITY:TU000037   \n",
       "\n",
       "                 source  \n",
       "30863  locations_1.json  \n",
       "36862  locations_0.json  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find duplicate IDs\n",
    "\n",
    "dup_id = climate_data_df[climate_data_df.duplicated(subset='id', keep=False)]\n",
    "\n",
    "dup_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deleting the duplicate row\n",
    "climate_data = climate_data_df.drop(30863)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mindate</th>\n",
       "      <th>maxdate</th>\n",
       "      <th>name</th>\n",
       "      <th>datacoverage</th>\n",
       "      <th>id</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [mindate, maxdate, name, datacoverage, id, source]\n",
       "Index: []"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rechecking for duplicates\n",
    "dups2 = climate_data[climate_data.duplicated()] # filters Climate df to return only rows where .duplicated() is True\n",
    "dups2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All entries in the 'id' column are unique\n"
     ]
    }
   ],
   "source": [
    "# secondary check for unique\n",
    "\n",
    "if climate_data['id'].is_unique:\n",
    "    print(\"All entries in the 'id' column are unique\")\n",
    "else:\n",
    "    print(\"There are duplicate entries in the 'id' column\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
